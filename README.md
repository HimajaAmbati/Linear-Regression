Linear Regression

# ğŸ“ˆ Linear Regression 

This folder contains my work on **Linear Regression**, one of the most fundamental algorithms in Machine Learning.It is also a supervised learning algorithm.
I implemented and explored it using the **House Price Prediction Dataset** from Kaggle.

---

## ğŸ§  What I Learned

### ğŸ“˜ Concept
- Regression predicts **continuous values**.
- Linear Regression finds the **best-fitting line** that minimizes the error between actual and predicted outputs.

### âš™ï¸ Math Behind It
         y=m*x+b
         w=w-a*(dL/dw)
         b=b-a*(dL/db)
### ğŸ” Training Method
- Implemented **Gradient Descent** manually to minimize the loss.
- Learned how **learning rate (Î±)** affects convergence.

### ğŸ§© Regularization
- Learned about **overfitting** and implemented:
  - **Ridge Regression (L2 penalty)**  
  - **Lasso Regression (L1 penalty)**

---

## ğŸ’» Code & Implementation

- I used **Google Colab** for experimentation.
- My notebook covers:
  - Linear Regression (from scratch)
  - Linear Regression (using `scikit-learn`)
  - Regularization techniques (Ridge, Lasso)
  - Visualization (Actual vs Predicted, Residuals, Heatmap)

---

## ğŸ“Š Evaluation Metrics
- **Mean Squared Error (MSE)**
- **Root Mean Squared Error (RMSE)**
- **RÂ² Score**

---

## ğŸ§© Visualization Examples
- Best-fit line between area and price  
- Residual error plots  

---

## ğŸ§  Key Takeaways
- Understood the math behind **Linear Regression**.
- Learned to implement ML logic **from scratch** and compare with **library results**.
- Understood the role of **regularization** in avoiding overfitting

**Author:** *Ambati Lakshmi Himaja*  
ğŸ“… *Part of my â€œLearning MLâ€ series*  

